<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>  SVMs  </title>
        <link rel="stylesheet" href="../style.css">

        <script src="./js/multicanv.js"></script>
        <script src="../import-colors.js"></script>
        <script src="./js/svm.js"></script>

        <link rel="shortcut icon" type="image/x-icon" href="../favicon.ico?v=2">

        <!-- Math rendering -->
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </head>
    <body>
        <p id="date"> September 4, 2023 </p>
        <h1> Support Vector Machines </h1>
        
        <canvas id="demo"></canvas>
        <div id="demo-controls">
            <input type="range" id="demo-hardness" min=".7" max="7" step=".05" value="2.25">
        </div>

        <p>
            Support Vector Machines (SVMs) are a type of AI. 
            They are simpler compared to neural networks, as they only seperate data linearly.
        </p>
        <p>
            The animation above shows the training process for an SVM learning
            to seperate the blue points from the orange. You can Move each point around 
            to get a feel of how the AI reacts.
        </p>
        <p>
            The faint gray lines either side of the dividing line represent
            the <i>margin</i>. This is how the SVM decides which line is 
            best to seperate the data. The best line is chosen as the one that 
            allows for the largest margin without hitting a data point.
        </p>

        <h2> Margin Hardness </h2>
        <p>
            The Hardness of the margin refers to how leniant the model is to 
            outliers, in effect specifying how closely to fit the line to the data.
            In some cases this can lead to better performance, as it allows 
            control over weather you want the model to over or underfit, and by 
            how much.
        </p>
        <p>
            The margin Hardness can be controled in the first demo using the slider
            below it.
        </p>

        <h2>The Kernel Trick</h2>
        <p>
            Since linearly seperable data is scarce in practice, SVM's are often 
            used with a function applied to the data before they are processed, 
            and in this way they make the data linearly seperable again, by adding an 
            extra dimension to the data.
        </p>

        <canvas id="kernel_trick"></canvas>
        <div id="kernel_trick-controls">
            <button id="apply" class="center">Apply Kernel Function</button>
        </div>

        <p>
            here the data is  initially one dimensional, and the second dimension is 
            added by the kernel function, allowing the AI to find a line to seperate 
            the two classes where it wasn't possible before.
        </p>

        <h2>The Maths</h2>
        <p>
            SVM's generally use gradient descent to find the seperating line.
            In order to define a general seperating line in many dimensions
            (a hyperplane) the line is defined using the dot product, meaning any 
            point perpendicular to the line is counted as part of the border.
        </p>
        <p>
            specifically, the line is defined as 

            $$ \vec{w} \cdot \vec{x} + b = 0 $$

            Where \( \vec{w} \) is the weights of the SVM (defining the normal), 
            \( \vec{x} \) is the point in space, and \(b\) is the bias.
            The margin is determined by the length of the vector \( \vec{w} \),
            and the lines for each margin are in fact defined as 
            \( \vec{w} \cdot \vec{x} + b = \pm 1 \)
            This formula generalizes to higher dimensions (more features) easily, defining 
            a plane in 3D, and a hyperplane in 4D and so on. 
        </p>

        <canvas id="hyperplane"></canvas>

        <p>
            The SVM's predictions then are based on the side of the hyperplane that
            a given sample lands on. This can be tested using the sign of the expression above.
            Each of the two classes are labelled as either class "1" or "-1".
        </p>

        <h2>Training</h2>
        <p>
            The SVM is trained using gradient descent, typically using the hinge loss 
            function, defined as follows:

            $$ \text{loss}(\vec{x}) = \begin{cases} 0 & y (\vec{w} \cdot \vec{x} - b)=0 \\ 1-y(\vec{w} \cdot \vec{x} - b) & \text{otherwise} \end{cases}  $$
            $$ y = \pm1 \text{ (the label)} $$

            This is just a way to encode the margin, since that is what is being maximised, 
            and weather a data point is on the correct side of the line to be classified properly
            (handled by the multiplication by \(y\)).
        </p>

    </body>
</html>
